到目前为止，在对系统的研究中，都依赖于一个简单的计算机系统模型，CPU 执行指令，而存储器系统为 CPU 存放指令和数据。在简单模型中，存储器系统是一个线性的字节数组，而 CPU 能够在一个常数时间内访问每个存储器位置。虽然迄今为止这都是一个有效的模型，但是它并没有反映现代系统实际工作的方式。

实际上，**存储器系统(memory system)** 是一个具有不同容量、成本和访问时间的存储设备的层次结构。CPU 寄存器保存着最常用的数据。靠近 CPU 的小的、快速的 **高速缓存存储器(cache memory)** 作为一部分存储在相对慢速的主存储器(main memory)中数据和指令的缓冲区域。主存缓存存储在容量较大的、慢速磁盘上的数据，而这些磁盘常常又作为存储在通过网络连接的其他机器的磁盘或磁带上的数据的缓冲区域。

存储器层次结构是可行的，这是因为与下一个更低层次的存储设备相比来说，一个编写良好的程序倾向于更频繁地访问某一个层次上的存储设备。所以，下一层的存储设备可以更慢速一点，也因此可以更大，每个比特位更便宜。整体效果是一个大的存储器池，其成本与层次结构底层最便宜的存储设备相当，但是却以接近于层次结构顶部存储设备的高速率向程序提供数据。

同时，存储器的层次结构对应用程序的性能有巨大的影响，因为不同存储位置访问成本差异巨大。
为此，程序员需要理解存储器层次结构，以在编写程序时，使得它们的数据项存储在层次结构中较高的地方，以便 CPU 能更快地访问到。
而这围绕着计算机程序的一个基本属性：**局部性（locality）** -- 具有良好局部性的程序倾向于一次又一次地访问相同的数据项集合，或是倾向于访问邻近的数据项集合。具有良好局部性的程序比局部性差的程序更多地倾向于从存储器层次结构中较高层次处访问数据项，因此运行得更快。

在本章中，将会讲述基本的存储技术 - SRAM 存储器、DRAM 存储器、ROM 存储器以及旋转的和固态的硬盘 -- 并描述它们是如何被组织成层次结构的。特别地，将主要讲述高速缓存存储器，它是作为 CPU 和主存之间的缓存区域，因为它们对应用程序性能的影响最大。将展示如何分析 C 程序的局部性，并且介绍改进程序中局部性的技术。还将讲述一种描绘某台机器上存储器层次结构的性能的方法，称为 “存储器山（memory mountain）”，它展示出读访问时间是局部性的一个函数。

---
# 6.1 存储技术
计算机技术的成功很大程度上源自于存储技术的巨大进步。

---
## 6.1.1 随机访问存储器
随机访问存储器（Random-Access Memory，RAM）分为两类：静态的和动态的。静态 RAM（SRAM）比动态 RAM（DRAM）更快，但也贵得多。SRAM 用来作为高速缓存存储器，既可以在CPU芯片上，也可以在片下。DRAM 用来作为主存以及图形系统的帧缓冲区。

### 1. 静态 RAM
SRAM 将每个位存储在一个 **双稳态的（bistable）** 存储器单元里。每个单元是用一个六晶体管电路来实现的。这个电路有这样一个属性，它可以无限期地保持在两个不同的电压配置（configuration）或状态（state）之一。其他任何状态都是不稳定的 -- 如果在不稳定状态，电路会迅速地转移到两个稳定状态中的一个。这样一个存储器单元类似于图 6-1 中画出的倒转的钟摆。
![[NoteAboutStudy/attachments/Pasted image 20251104215528.png]]

由于 SRAM 存储器单元的双稳态特性，只要有电，它就会永远地保持它的值。即使有干扰（例如电子噪音）来扰乱电压，当干扰消除时，电路就会恢复到稳定值。

### 2. 动态 RAM
DRAM 将每个位存储为对一个电容的充电。而这个电容非常小，因此 DRAM 存储器可以制造得非常密集 -- 每个单元由一个电容和一个访问晶体管组成。

但是，与 SRAM 不同，DRAM 存储器单元对干扰非常敏感。当电容的电压被扰乱之后，它就永远不会恢复了。暴露在光线下会导致电容电压改变。实际上，数码照相机和摄像机中的传感器本质上就是 DRAM 单元的阵列。

图 6-2 总结了 SRAM 和 DRAM 存储器的特性。只要有供电，SRAM 就会保持不变。与 DRAM 不同，它不需要刷新。SRAM 的存取比 DRAM 快。SRAM 对诸如光和电噪声这样的干扰不敏感。代价是 SRAM 单元比 DRAM 单元使用更多的晶体管，因而密集度低，而且更贵，功耗更大。
![[NoteAboutStudy/attachments/Pasted image 20251104220048.png]]

### 3. 传统的 DRAM
DRAM 芯片中的单元（位）被分成 $d$ 个 **超单元（supercell）**，每个超单元都由 $w$ 个 DRAM 单元组成。一个 $d×w$ 的 DRAM 总共存储了 $dw$ 位信息。超单元被组织成一个 $r$ 行 $c$ 列的长方形阵列，这里 $rc=d$。每个超单元有形如 $(i,j)$ 的地址，这里 $i$ 表示行，而 $j$ 表示列。如图 6-3 为例子。

![[NoteAboutStudy/attachments/Pasted image 20251104221149.png]]

> [!tip]- 关于 超单元 的注释
> 存储领域从来没有为 DRAM 的阵列元素确定一个标准的名字。计算机构架师倾向于称之为“单元”，使这个术语具有 DRAM 存储单元之意。电路设计者倾向于称之为“字”，使之具有主存一个字之意。为了避免混淆，本书采用了无歧义的术语“超单元”。

每个 DRAM 芯片被连接到某个称为 **内存控制器（memory controller）** 的电路，这个电路可以一次传送 $w$ 位到每个 DRAM 芯片或一次从每个 DRAM 芯片传出 $w$ 位。为了读出超单元 $(i,j)$ 的内容，内存控制器将行地址 $i$ 发送到 DRAM，然后是列地址 $j$。DRAM 把超单元 $(i,j)$ 的内容发回给控制器作为响应。行地址 $i$ 称为 RAS（Row Access Strobe，行访问选通脉冲）请求。列地址 $j$ 称为CAS（Column Access Strobe，列访问选通脉冲）请求。注意，RAS 和 CAS 请求共享相同的 DRAM 地址引I脚。

![[NoteAboutStudy/attachments/Pasted image 20251104221216.png]]

电路设计者将 DRAM 组织成二维阵列而不是线性数组的一个原因是降低芯片上地址引脚的数量。

### 4. 内存模块
DRAM 芯片封装在 **内存模块（memorymodule）** 中，它插到主板的扩展槽上。图 6-5 展示了一个内存模块的基本思想。

![[NoteAboutStudy/attachments/Pasted image 20251104221723.png]]
示例模块用 8 个 64Mbit 的 8M×8 的 DRAM 芯片，总共存储 64MB（兆字节），这 8 个芯片编号为 0~7。每个超单元存储主存的一个字节，而用相应超单元地址为（i，j）的 8 个超单元来表示主存中字节地址 A 处的 64 位字。在图 6-5 的示例中，DRAM 0 存储第一个（低位）字节，DRAM 1 存储下一个字节，依此类推。
要取出内存地址 A 处的一个字，内存控制器将 A 转换成一个超单元地址（i，j），并将它发送到内存模块，然后内存模块再将 i 和 j 广播到每个 DRAM。作为响应，每个 DRAM 输出它的（i，j）超单元的 8 位内容。模块中的电路收集这些输出，并把它们合并成一个 64 位字，再返回给内存控制器。通过将多个内存模块连接到内存控制器，能够聚合成主存。在这种情况中，当控制器收到一个地址 A 时，控制器选择包含 A 的模块 k，将 A 转换成它的（i，j）的形式，并将（i，j）发送到模块 k。

### 5. 增强的 DRAM
有许多种 DRAM 存储器，而生产厂商试图跟上迅速增长的处理器速度，市场上就会定期推出新的种类。每种都是基于传统的 DRAM 单元，并进行一些优化，提高访问基本 DRAM 单元的速度。

|类型|优化机制|说明|
|---|---|---|
|**FPM DRAM**（Fast Page Mode）|行缓冲区连续访问|读同一行多个单元，只需一次 RAS + 多次 CAS，比传统 DRAM 快。|
|**EDO DRAM**（Extended Data Out）|CAS 信号间隔缩短|对 FPM DRAM 的增强，连续访问速度更快。|
|**SDRAM**（Synchronous DRAM）|与控制器同步时钟|使用外部时钟同步读写操作，比异步 DRAM 输出更快。|
|**DDR SDRAM**（Double Data Rate）|双倍数据速率|利用时钟上升沿和下降沿，速度翻倍；进一步有 DDR2、DDR3、DDR4，通过增加预取缓冲提升带宽。|
|**VRAM**（Video RAM）|并行读写|用于图形缓冲区，可同时读（显示屏幕）和写（更新帧缓冲），输出通过移位整个缓冲区。|

### 6. 非易失性存储器
如果断电，DRAM 和 SRAM 会丢失它们的信息，从这个意义上说，它们是 **易失的（volatile**）。另一方面，**非易失性存储器（nonvolatile memory）** 即使是在关电后，仍然保存着它们的信息。现在有很多种非易失性存储器。由于历史原因，虽然 ROM 中有的类型既可以读也可以写，但是它们整体上都被称为 **只读存储器（Read-OnlyMemory，ROM）**。 ROM 是以它们能够被重编程（写）的次数和对它们进行重编程所用的机制来区分的。
- **PROM（Programmable ROM，可编程ROM）** 只能被编程一次。PROM 的每个存储器单元有一种熔丝（fuse），只能用高电流熔断一次。
- **可擦写可编程 ROM（Erasable Programmable ROM，EPROM）** 有一个透明的石英窗口，允许光到达存储单元。紫外线光照射过窗口，EPROM 单元就被清除为 0。对 EPROM 编程是通过使用一种把 1 写入 EPROM 的特殊设备来完成的。EPROM 能够被擦除和重编程的次数的数量级可以达到 1000 次。
- **电子可擦除 PROM（Electrically Erasable PROM，EEPROM）** 类似于 EPROM，但是它不需要一个物理上独立的编程设备，因此可以直接在印制电路卡上编程。EEPROM 能够被编程的次数的数量级可以达到 $10^5$ 次。
- **闪存（flash memory）** 是一类非易失性存储器，基于 EEPROM，它已经成为了一种重要的存储技术。闪存无处不在，为大量的电子设备提供快速而持久的非易失性存储，包括数码相机、手机、音乐播放器、PDA 和笔记本、台式机和服务器计算机系统。在 6.1.3 节中，将会仔细研究一种新型的基于闪存的磁盘驱动器，称为 **固态硬盘（Solid State Disk，SSD）**，它能提供相对于传统旋转磁盘的一种更快速、更强健和更低能耗的选择。

存储在 ROM 设备中的程序通常被称为 **固件（firmware）**。当一个计算机系统通电以后，它会运行存储在 ROM 中的固件。一些系统会在固件中提供了少量基本的输入输出函数。

### 7. 访问主存
数据流通过称为 **总线（bus）** 的共享电子电路在处理器和 DRAM 主存之间来来回回。每次 CPU 和主存之间的数据传送都是通过一系列步骤来完成的，这些步骤称为 **总线事务（bus transaction）**。**读事务（read transaction）** 从主存传送数据到 CPU。**写事务（write trans action）** 从 CPU 传送数据到主存。

总线是一组并行的导线，能携带地址、数据和控制信号。取决于总线的设计，数据和地址信号可以共享同一组导线，也可以使用不同的。同时，两个以上的设备也能共享同一总线。控制线携带的信号会同步事务，并标识出当前正在被执行的事务的类型。

![[NoteAboutStudy/attachments/Pasted image 20251104223407.png]]

如图 6-6 展示了一个示例计算机系统的配置。主要部件是 CPU 芯片、将称为 I/O 桥接器（I/O bridge）的芯片组（其中包括内存控制器），以及组成主存的 DRAM 内存模块。这些部件由一对总线连接起来，其中一条总线是 **系统总线（system bus）**，它连接 CPU 和 I/O 桥接器，另一条总线是 **内存总线（memory bus）**，它连接 I/O 桥接器和主存。I/O 桥接器将系统总线的电子信号翻译成内存总线的电子信号。正如所看到的那样，I/O 桥也将系统总线和内存总线连接到 I/O 总线，像磁盘和图形卡这样的 I/O 设备共享 I/O 总线。不过现在，将注意力集中在内存总线上。

CPU 执行指令需要从主存读取或写入数据。而主存比 CPU 寄存器慢很多，需要通过 **系统总线** 和 **内存总线** 与 CPU 通信。CPU 芯片上的 **总线接口（bus interface）** 负责协调这些读写事务。

#### 读主存（Load）的流程
以指令 `movq A, %rax` 为例：
1. **发起读请求**  
    CPU 将地址 `A` 放到 **系统总线**上，I/O 桥将信号传到 **内存总线**。
2. **主存响应**  
    DRAM 识别地址，从内存单元取出对应的数据字，并写入内存总线。
3. **数据回传 CPU**  
    I/O 桥把内存总线信号转换回系统总线信号，CPU 从总线读取数据并放入寄存器 `%rax`。
```流程总结
CPU → 系统总线 → I/O桥 → 内存总线 → DRAM → 内存总线 → I/O桥 → 系统总线 → CPU
```
![[NoteAboutStudy/attachments/Pasted image 20251104224652.png]]

#### 写主存（Store）的流程
以指令 `movq %rax, A` 为例：
1. **发起写请求**  
    CPU 将目标地址 `A` 放到系统总线上，主存知道地址并等待数据。
2. **传输数据**  
    CPU 将寄存器 `%rax` 的内容写入系统总线。
3. **写入 DRAM**  
    主存从内存总线读取数据，并将其存储到 DRAM 的指定位置。
```流程总结
CPU（数据） → 系统总线 → I/O桥 → 内存总线 → DRAM
```
![[NoteAboutStudy/attachments/Pasted image 20251105152511.png]]

## 6.1.2 磁盘存储
磁盘是广为应用的保存大量数据的存储设备，存储数据的数量级可以达到几百到几千千兆字节，而基于 RAM 的存储器只能有几百或几千兆字节。不过，从磁盘上读信息的时间为毫秒级，比从 DRAM 读慢了 10 万倍，比从 SRAM 读慢了 100 万倍。

### 1. 磁盘构造
磁盘由多个 **盘片(platter)** 堆叠构成。每个盘片有两面或者称为 **表面(surface)** 覆盖着磁性记录材料。盘片中央有一个旋转的 **主轴(spindle)**，它使得盘片以固定的 **旋转速率** 旋转，通常是 5400 ~ 15000 转每分钟(Revolution Per Minute, RPM)。磁盘通常包含一个或多个这样的盘片，并封装在一个密封的容器内。

![[NoteAboutStudy/attachments/Pasted image 20251105153512.png]]
图 6-9a 展示了一个典型的磁盘表面的结构。每个表面是由一组称为 **磁道(track)** 的同心圆组成的。每个磁道被划分为一组 **扇区(sector)**。每个扇区包含相等数量的数据位（通常是 512 字节)，这些数据编码在扇区上的磁性材料中。扇区之间由一些 **间隙(gap)** 分隔开，这些间隙中不存储数据位。间隙存储用来标识扇区的格式化位。

磁盘是由一个或多个叠放在一起的盘片组成的，它们被封装在一个密封的包装里，如图 6-9b 所示。整个装置通常被称为 **磁盘驱动器(disk drive)**，通常简称为 **磁盘(disk)**。有时，会称磁盘为 **旋转磁盘(rotating disk)**，以使之区别于基于闪存的 **固态硬盘(SSD)**，SSD 是没有移动部分的。

磁盘制造商通常用术语 **柱面(cylinder)** 来描述多个盘片驱动器的构造，这里，柱面是所有的盘片表面上到主轴中心的 距离相等 的磁道的集合。例如，如果一个驱动器有三个盘片和六个面，每个表面上的磁道的编号都是一致的，那么柱面就是 6 个磁道的集合。

### 2. 磁盘容量
一个磁盘上可以记录的最大位数称为它的 **最大容量**，或者简称为 **容量**。磁盘容量是由以下技术因素决定的：
- **记录密度(recording density)(位/英寸)**：磁道一英寸的段中可以放入的位数。
- **磁道密度(track density)(道/英寸)**：从盘片中心出发半径上一英寸的段内可以有的
磁道数。
- **面密度(areal density)(位/平方英寸)**：记录密度与磁道密度的乘积。

因为磁盘制造商不懈地努力以提高面密度从而提高容量，所以面密度每隔几年就会翻倍。
而最初的磁盘，是在面密度很低的时代设计的，将每个磁道分为数目相同的扇区，扇区的数目是由最靠内的磁道能记录的扇区数决定的。为了保持每个磁道有固定的扇区数，越往外的磁道扇区间隙越大。在面密度相对比较低的时候，这种方法还算合理。
不过，随着面密度的提高，扇区之间的间隙（那里没有存储数据位）变得不可接受地大。因此，现代大容量磁盘使用一种称为 **多区记录(multiple zone recording)** 的技术，它将磁盘的柱面（磁道组成的同心圆集合）分成多个 **记录区**，每个记录区包含一批连续的柱面，一个区内部所有磁道扇区数一样（以最里面的磁道所能包含的扇区数决定），不同区之间扇区数可以不同（通常越往外越多）。

下面的公式给出了一个磁盘容量的计算：$$磁盘容量 = \frac{字节数}{扇区} ×\frac{平均扇区数}{磁道}×\frac{磁道数}{表面}×\frac{表面数}{盘片}×\frac{盘片数}{磁盘}$$ 也就是，磁盘容量 = 每扇区字节数 × 每磁道平均扇区数 × 每面磁道数 × 每盘片面数（这个目前只能是 2） × 每磁盘盘片数。

> [!important]+ 关于容量
> 不幸地，像 K(kilo)、M(mega)、G(giga) 和 T(tera) 这样的前缀的含义依赖于上下文。对于与 DRAM 和 SRAM 容量相关的计量单位，通常 K=$2^{10}$，M=$2^{20}$，G=$2^{30}$，而 T=$2^{40}$。对于与像磁盘和网络这样的 I/O 设备容量相关的计量单位，通常 K=$10^3$， M=$10^6$，G=$10^9$，而 T=$10^{12}$。速率和吞吐量常常也使用这些前缀。
> 幸运地，对于通常依赖的不需要复杂计算的估计值，无论哪种假设在实际中相对差别不大。

### 3. 磁盘操作
磁盘通过 **读/写头** 来读写磁性表面的数据。读/写头固定在 **传动臂（Actuator Arm）** 上，通过机械移动定位数据位置。这种将读/写头移动到指定磁道的过程叫 **寻道（Seek）**。当磁盘旋转让相应的位经过读/写头时，读/写头就能读取或写入数据。有多个盘片的磁盘针对每个盘面都有一个独立的读/写头，如图 6-10b 所示。读/写头垂直排列，一致行动。在任何时刻，所有的读/写头都位于同一个柱面上。

![[NoteAboutStudy/attachments/Pasted image 20251105205239.png]]

如果盘面上出现一粒细小的灰尘，则会让读/写头停下来，产生所谓的读/写头 **碰撞(head crash)**，为此，磁盘总是密封包装。

磁盘以扇区大小的块来读写数据。对扇区的 **访问时间(access time)** 有三个主要的部分：
- **寻道时间(seek time)**：移动传动臂到所要读取的包含目标扇区的磁道上的时间称为**寻道时间**。该时间依赖于读/写头之前的位置和传动臂移动的速度；
- **旋转时间(rotational latency)**：当读/写头移动到了所要到达的磁道，还需要等待目标扇区的第一个位转到读/写头下，这个时间或者说延迟依赖于读/写头到达时盘面的位置以及磁盘的旋转速度。最坏的情况下，要等待旋转一圈，因此最大旋转时间为磁盘旋转一圈的时间；
- **传送时间(transfer time)**：当目标扇区的第一个位位于读/写头下时，驱动器就可以开始读或者写该扇区的内容了，读/写这个扇区所需的时间称为传送时间，它依赖于磁盘旋转速度和每条磁道的扇区数目；

### 4. 逻辑磁盘块
现代硬盘内部结构复杂（多盘面、多区记录等），但**为了让操作系统简单使用**，磁盘提供了一个**线性逻辑块地址空间**，隐藏实际复杂结构：
> **磁盘 = 从 0 到 B–1 的逻辑块序列（每块 = 1 扇区）**

磁盘内部封装有一个 **磁盘控制器**，负责：
- 维护 **逻辑块号 → 物理位置** 的映射；
- 执行磁盘寻址 和 数据传输；

I/O 操作的流程 - 读操作：
1. OS 发命令：**读逻辑块 N**；
2. 控制器查表：逻辑块 → 物理位置（盘面，磁道，扇区组成的三元组，这个三元组唯一地标识了对应的物理扇区）；
3. 控制器驱动硬件：
    - 移动读头到目标磁道（寻道）
    - 等扇区转到头下（旋转延迟）
4. 将数据读入控制器缓冲区 → 主存；

> [!important]+ 格式化的磁盘容量
> 磁盘控制器必须对磁盘进行格式化，然后才能在该磁盘上存储数据。格式化包括用标识扇区的信息填写扇区之间的间隙，标识出表面有故障的柱面并且不使用它们，以及在每个区中预留出一组柱面作为备用，如果区中一个或多个柱面在磁盘使用过程中坏掉了，就可以使用这些备用的柱面。因为存在着这些备用的柱面，所以磁盘制造商所说的格式化容量比最大容量要小。

### 5. 连接 I/O 设备
**I/O设备（显卡、显示器、鼠标、键盘、磁盘等）**  通过 **I/O总线**（典型如 **PCI** -- `Intel` 的外围设备互连 Peripheral Component Interconnect 总线）连接 CPU 和主存。

虽然 I/O 总线比 系统总线和内存总线(与 CPU 相关) 慢，但是它可以容纳种类繁多的第三方 I/O 设备。例如，在图 6-11 中，有三种不同类型的设备连接到总线。
![[NoteAboutStudy/attachments/Pasted image 20251105214843.png]]
- **通用串行总线(Universal Serial Bus, USB)** 控制器是一个连接到 USB 总线的设备的中转机构，USB 总线是一个广泛使用的标准，连接各种外围 I/O 设备（USB 3.0 最大带宽：**625MB/s** / USB 3.1 最大带宽：**1250MB/s**）；
- **显卡(GPU)** -- 图形适配器 包含硬件和软件逻辑，负责图形渲染；
- **主机总线适配器** -- 也成为磁盘控制器(HBA) 将一个或多个磁盘连接到 I/O 总线， 使用的是一个特别的 **主机总线接口** 定义的通信协议。两个最常用的接口是 SCSI 和 SATA，SCSI 比 SATA 更快但也更贵，前者可以支持多个磁盘驱动器，后者只能支持一个；
- 其他的设备，例如 **网络适配器** 等，可以通过将适配器插入到主板上空的 **扩展槽** 中，从而连接到 I/O 总线；

> [!tip]- I/O 总线设计进展
> 图 6-11 中的 I/O 总线是一个简单的抽象，使得可以具体描述但又不必和某个系统的细节联系过于紧密。它是基于外围设备互联(Peripheral Component Interconnect, PCI)总线的，在 2010 年前使用非常广泛。PCI 模型中，系统中所有的设备共享总线，一个时刻只能有一台设备访问这些线路。在现代系统中，共享的 PCI 总线已经被 PCEe(PCI express)总线取代，PCIe 是一组高速串行、通过开关连接的点到点链路，类似于将在第 11 章中学习到的开关以太网。PCIe总线，最大吞吐率为 I6GB/s，比 PCI 总线快一个数量级，PCI 总线的最大吞吐率为 533MB/s。除了测量出的 I/O 性能，不同总线设计之间的区别对应用程序来说是不可见的，所以在本书中，只使用简单的共享总线抽象。

### 6. 访问磁盘
如图 6-12 简洁概要的总结了当 CPU 从磁盘读数据时发生的步骤。

![[NoteAboutStudy/attachments/Pasted image 20251105221152.png]]
CPU 通过 **内存映射 I/O(memory-mapped I/O)** 来向 I/O 设备发射命令，如图 6-12a。在使用这个的系统中：
- 地址空间中有一块地址是用于与 I/O 设备通信的，每个这样的地址称为 I/O **端口(port)**，CPU 通过往这些端口写值来发松命令；
- 当一个设备连接到总线时，它与一个或多个端口相关联（或它被映射到一个或多个端口）；

例：磁盘控制器映射到 `0xa0`  
CPU 写 三条指令到该端口：
1. 写 命令字（读操作 / 是否中断）；
2. 写 逻辑块号；
3. 写 目标内存地址；

在磁盘控制器收到来自 CPU 的读命令之后，它将逻辑块号翻译成一个扇区地址，读该扇区的内容，然后将这些内容直接传送到主存，不需要 CPU 的干涉（图 6-12b)。

设备可以自己执行读或者写总线事务而不需要 CPU 干涉的过程，称为 **直接内存访问(Direct Memory Access, DMA)**。这种数据传送称为 **DMA 传送(DMA transfer)**。

> 在此期间，CPU 可以空出时间去干别的，等待只会浪费指令周期。

在 DMA 传送完成，磁盘扇区的内容被安全地存储在主存中以后，磁盘控制器通过给 CPU 发送一个中断信号来通知 CPU(图 6-12c)。

基本思想是中断会发信号到 CPU 芯片的一个外部引脚上。这会导致 CPU 暂停它当前正在做的工作，跳转到一个操作系统中断例程。这个程序会记录下 I/O 已经完成，然后将控制返回到 CPU 被中断的地方。

## 6.1.3 固态硬盘
固态硬盘(Solid State Disk，SSD) 是一种基于闪存的存储技术（常见 6.1.1），在某种情况下是传统旋转磁盘的强力替代产品。如图 6-13 展示了它。

![[NoteAboutStudy/attachments/Pasted image 20251105222841.png]]

SSD 封装插到 I/O 总线上标准硬盘插槽（通常是 USB 或 SATA)中，行为就和其他硬盘一样，处理来自 CPU 的读写逻辑磁盘块的请求。一个 SSD 封装由一个或多个闪存芯片和 **闪存翻译层(flash translation layer)** 组成，闪存芯片替代传统旋转磁盘中的机械驱动器，而闪存翻译层是一个硬件/固件设备，扮演与磁盘控制器相同的角色，将对逻辑块的请求翻译成对底层物理设备的访问。

> 行为像硬盘，本质是闪存阵列 + 地址映射硬件。

并且，读 SSD 比写要快。这由底层闪存基本属性决定的。如图 6-13 所示，一个闪存由 B 个块的序列组成，每个块由 P 页组成。

通常，页的大小是 512 字节~4KB，块是由 32~128 页组成的，块的大小为 16KB~512KB。

数据是以页为单位读写的。只有在一页所属的块整个被擦除之后，才能写这一页（通常是指该块中的所有位都被设置为 1）。不过，一旦一个块被擦除了，块中每一个页都可以不需要再进行擦除就写一次。在大约进行 $10^5$ 次重复写之后，块就会磨损坏。一旦一个块磨损坏之后，就不能再使用了。

写很慢有两个原因：
1. 擦除块需要相对较长的时间，1s 级的，比访问页所需时间要高一个数量级；
2. 如果写操作试图修改一个包含已经有数据（也就是不是全为 1)的页 p，那么这个块中所有带有用数据的页都必须被复制到一个新（擦除过的）块，然后才能进行对页 p 的写。
制造商已经在闪存翻译层中实现了复杂的逻辑，试图抵消擦写块的高昂代价，最小化内部写的次数，但是随机写的性能不太可能和读一样好。

SSD vs 机械硬盘
1. 优势：
	- **无机械结构 → 超快随机访问**；
	- 低功耗、抗震；
2. 劣势：
	- 会磨损（但有闪存翻译层中的 **平均磨损 wear leveling** 逻辑通过将擦除平均分布在所有块上来最大化每个块的寿命）；
	- **价格更贵**（约 ×30 / 字节）因此常用的容量较小（约 ÷100）；
	- 不过，随着 SSD 变得越来越受欢迎，它的价格下降得非常快，而两者的价格差也在减少；
3. 趋势：
	- 手机/平板/音乐设备 → **完全替代HDD**；
	- 笔记本/桌面/服务器 → 快速普及；

> 未来主流 + HDD长期用于大容量冷存储

## 6.1.4 存储技术趋势
- **不同存储技术存在“性能—成本”的折中**
    - SRAM > DRAM > SSD > 磁盘；
    - 越快 → 越贵；越慢 → 越便宜、容量更大；
        
- **各类存储技术的性能和成本改善速度不同**
    - 有的技术“提速容易”，有的“降价快但变快难”；
    - 总的来说，提升存储密度（从而降低成本）远比缩短访问时间容易得多；

- **DRAM 和磁盘的性能滞后于 CPU 的性能**
	- CPU 提升更快：1985→2010年间，CPU周期时间提高约 **500×**；若计入多核，则达 **2000×**；
	- 虽然 SRAM 的性能滯后于 CPU 的性能，但还是在保持增长；
	- DRAM 与磁盘性能严重滞后
	    → CPU 与主存/磁盘间的速度差距越来越大；
	    → 出现 “存储墙”（Memory Wall） 问题；
	- 2003 年起，多核架构普及，性能不再单纯看“延迟”，而是取决于 **吞吐量**，多个处理器核并发地发请求）；

为此，现代计算机频繁地使用基于 SRAM 的高速缓存(cache)，如 6.4 节所述，来缓解 CPU 与主存的性能差距，之所以有效，是因为程序具有 **局部性（locality）** —— 即数据访问集中在某些范围内，下一节将阐述这一基本属性。

# 6.2 局部性
一个编写良好的计算机程序常常具有良好的 **局部性(locality)**。也就是，它们倾向于引用邻近于其他最近引用过的数据项的数据项，或者最近引用过的数据项本身。这种倾向性，被称为 **局部性原理(principle of locality)**，是一个持久的概念，对硬件和软件系统的设计和性能都有着极大的影响。

局部性通常有两种形式：
 - **时间局部性(temporal locality)**：在一个具有良好时间局部性的程序中，被引用过一次的内存位置很可能在之后被多次引用；
 - **空间局部性(spatial locality)**：在一个具有良好空间局部性的程序中，如果一个内存位置被引用过一次，那么程序很可能在之后引用附近的一个内存位置；
**一般而言，有良好局部性的程序比局部性差的程序运行得更快。**

现代计算机系统的各个层次，从硬件到操作系统、再到应用程序，它们的设计都利用了局部性：
 - 在硬件层，局部性原理允许计算机设计者通过引入称为高速缓存存储器的小而快速的存储器来保存最近被引用的指令和数据项，从而提高对主存的访问速度；
 - 在操作系统级，局部性原理允许系统使用主存作为虚拟地址空间最近破引用块的高速缓存。类似地，操作系统用主存来缓存磁盘文件系统中最近被使用的磁盘块；
 - 在应用程序的设计中，Web 浏览器将最近被引用的文档放在本地磁盘上，利用的就是时间局部性。大容量的 Web 服务器将最近被请求的文档放在前端磁盘高速缓存中，这些缓存能满足对这些文档的请求，而不需要服务器的任何干预；

---
## 6.2.1 对程序数据引用的局部性
这一节从三个代码去讲对程序数据引用的局部性，在这之前，还要讲讲步长：

**步长（stride）**：两次连续访问所间隔的元素数量。
- 步长为 1：连续访问 → **顺序引用模式**（最优空间局部性）；
- 步长为 k：隔 k 个元素访问 → 空间局部性随 k 增大而下降；
**步长越大，缓存命中率越低，性能越差。**

### 一维数组求和
```c
int sumvec(int v[], int n) {
    int i, sum = 0;
    for (i = 0; i < n; i++)
        sum += v[i];
    return sum;
}
```
1. 变量 `sum`：
	- 在每次循环迭代都被引用一次，故有好的时间局部性；
	- `sum` 是标量，没有空间局部性；
2. 向量 `v`：
	- 向量 `v` 的元素是被顺序读取的，即顺序访问数组元素，有很好的空间局部性；
	- 因为每个向量元素只被访问一次，因此时间局部性很差；
**整体上局部性良好**（每个访问都至少利用了一种局部性）。

### 二维数组求和（按行访问）
```c
int sumarrayrows(int a[M][N]) {
    int i, j, sum = 0;
    for (i = 0; i < M; i++)
        for (j = 0; j < N; j++)
            sum += a[i][j];
    return sum;
}
```
双重嵌套循环按照 **行优先顺序(row major order)** 读数组的元素。也就是，内层循环读第一行的元素，然后读第二行，依此类推。函数 `sumarrayrows` 具有良好的空间局部性，因为它按照数组被存储的行优先顺序来访问这个数组。其结果是得到一个很好的步长为 1 的引用模式，具有良好的空间局部性。

### 二维数组求和（按列访问）
```c
int sumarraycols(int a[M][N]) {
    int i, j, sum = 0;
    for (j = 0; j < N; j++)
        for (i = 0; i < M; i++)
            sum += a[i][j];
    return sum;
}
```
函数 `sumarraycols` 的空间局部性很差，因为它按照列顺序来扫描数组，而不是按照行顺序。因为 C 数组在内存中是按照行顺序来存放的，结果就得到步长为 N 的引用模式 -- 实际访问间隔 = 一整行的大小。

## 6.2.2 取指令的局部性
程序运行时不仅要 “取数据”，还要不断 “取指令”（从内存中读取要执行的机器代码）。因此，也能够评价一个**程序的执行流**的局部性。

CPU 在执行时，会从内存中取出一条条指令，因此可以评估：
> “程序对指令的引用是否具有时间局部性或空间局部性”。

这对应着两种情况：
1. 时间局部性
	- **定义**：当某段指令被执行后，不久可能会再次执行。
	- **典型来源**：循环（loop）
	    - 例如：`for (i = 0; i < n; i++) sum += v[i];`
	    - 循环体的指令会被**重复多次执行**，具有良好的**时间局部性**；
2. 空间局部性
	- **定义**：当某条指令被取出后，下一条被取出的指令往往在内存中相邻。
	- **典型来源**：顺序执行的代码块
	    - 例如：`sum += v[i]; i++;`
	    - 程序的指令在内存中是顺序存放的，CPU 通常按照顺序执行（除非遇到跳转或函数调用），具有良好的**空间局部性**；

代码访问和数据访问最大的不同是：
>  **程序指令在运行时几乎不会被修改。**

数据在运行时可能被频繁读写（如数组或变量），但是指令在运行时**只读不写**：CPU只需要从内存取出指令，不会去修改这些指令的内容。

因此：**代码的局部性主要体现在 “取指令的模式” 上**（即执行顺序），**不会存在 “写指令” 的时间局部性**。

## 6.2.3 局部性小结
在这一节中，介绍了局部性的基本思想，还给出了量化评价程序中局部性的一些简单原则：
- 重复引用相同变量的程序有良好的时间局部性；
- 对于具有步长为 k 的引用模式的程序，步长越小，空间局部性越好。具有步长为的引用模式的程序有很好的空间局部性。在内存中以大步长跳来跳去的程序空间局部性会很差；
- 对于取指令来说，循环有好的时间和空间局部性。循环体越小，循环迭代次数越多，局部性越好；

# 6.3 存储器层次结构
6.1 节和 6.2 节描述了存储技术和计算机软件的一些基本的和持久的属性：
- 存储技术：不同存储技术的访问时间差异很大。速度较快的技术每字节的成本要比速度较慢的技术高，而且容量较小。CPU和主存之间的速度差距在增大。
- 计算机软件：一个编写良好的程序倾向于展示出良好的局部性。

硬件和软件的这些基本属性互相补充得很完美。它们这种相互补充的性质使人想到一种组织存储器系统的方法，称为 **存储器层次结构(memory hierarchy)**，所有的现代计算机系统中都使用了这种方法。图 6-21 展示了一个典型的存储器层次结构。

![[NoteAboutStudy/attachments/Pasted image 20251107151940.png]]

一般而言，从高层往底层走，存储设备变得更慢、更便宜和更大。在最高层(L0)，是少量快速的 CPU 寄存器，CPU 可以在一个时钟周期内访问它们。接下来是一个或多个小型到中型的基于 SRAM 的高速缓存存储器，可以在几个 CPU 时钟周期内访问它们。然后是一个大的基于 DRAM 的主存，可以在几十到几百个时钟周期内访问它们。接下来是慢速但是容量很大的本地磁盘。最后，有些系统甚至包括了一层附加的远程服务器上的磁盘，要通过网络来访问它们。

---
## 6.3.1 存储器层次结构中的缓存
**缓存（Cache）**：一个 “小而快” 的存储设备，用来暂时保存从 “大而慢” 的存储设备中取出的数据副本，它是一个 “**中间缓冲层**”，让系统**用更少的时间访问热点数据**。

> 存储器层次结构的每一层，都是下一层的缓存，每一层都用来 “加速访问” 下层的慢设备。

![[NoteAboutStudy/attachments/Pasted image 20251107184545.png]]
如图 6-22 展示了存储器层次结构中缓存的一般性概念，第 `k+1` 层的存储器被划分成连续的数据对象组块，称为 **块(block)**：
- 每个块都有唯一的地址或名字；
- 块可以是固定大小的（通常是这样的），也可以是可变大小的（例如存储在 Web 服务器上的远程 HTML 文件）；
- 第 k 层的存储器被划分成较少的块的集合，每个块的大小与 k+1 层的块的大小一样。在任何时刻，第 k 层的缓存包含第 k+1 层块的一个子集的副本；
- 数据总是以块大小为 **传送单元(transfer unit)** 在第 k 层和第 k+1 层之间来回复制的；
- 虽然在层次结构中任何一对相邻的层次之间块大小是固定的，但是其他的层次对之间可以有不同的块大小，一般而言，层次结构中较低层（离 CPU 较远）的设备的访问时间较长，因此为了补偿这些较长的访问时间，倾向于使用较大的块；

### 1. 缓存命中（Cache Hit）
当程序要访问的数据 **已经在缓存中存在** 时：
- 称为 “命中（hit）”；
- 数据直接从缓存中取出；
- 访问延迟显著降低。

### 2. 缓存不命中（Cache Miss）
当程序要访问的数据 **不在缓存中** 时：
- 称为 “不命中（miss）”；
- 缓存必须：
    1. 从下一层（更慢的存储）中取出包含要访问的数据的块；
    2. 若当前缓存已满，可能就会覆盖现存的一个块；
    3. 将新块放入缓存；
    4. 程序再访问它。

覆盖一个现存的块的过程称为 **替换(replacing)** 或 **驱逐(evicting)** 这个块。被驱逐的这个块有时也称为 **牺牲块(victim block)**。决定该替换哪个块是由缓存的 **替换策略(replacement policy)** 来控制的：
- 随机（Random）替换策略 会选择一个牺牲块；
- 最近最少使用（LRU, Least Recently Used）替换策略会选择最后被访问时间距现在最远的块 → 现实中最常见，因为它符合 “时间局部性”。

### 3. 缓存不命中的种类
如果第 k 层的缓存是空的，那么对任何数据对象的访问都会不命中。一个空的缓存有时被称为 **冷缓存(cold cache)**，此类不命中称为 **强制性不命中(compulsory miss)** 或 **冷不命中(cold miss)**。冷不命中很重要，因为它们通常是短暂的事件，不会在反复访问存储器使得缓存 **暖身(warmed up)** 之后的稳定状态中出现。

只要发生了不命中，第 k 层的缓存就必须执行某个 **放置策略(placement policy)**。确定把它从第 k+1 层中取出的块放在哪里：
- 最灵活的替换策略是允许来自第 k+1 层的任何块放在第 k 层的任何块中。对于存储器层次结构中高层的缓存（靠近 CPU)，它们是用硬件来实现的，而且速度是最优的，这个策略实现起来通常很昂贵，因为随机地放置块，定位起来代价很高；
- 因此，硬件缓存通常使用的是更严格的放置策略，这个策略将第 k+1 层的某个块限制放置在第 k 层块的一个小的子集中（有时只是一个块）；
这种限制性的放置策略会引起一种不命中，称为 **冲突不命中(conflict miss)**，在这种情况中，缓存足够大，能够保存被引用的数据对象，但是因为这些对象会映射到同一个缓存块，缓存会一直不命中。

程序通常是按照一系列阶段（如循环）来运行的，每个阶段访问缓存块的某个相对稳定不变的集合。当工作集的大小超过缓存的大小时，缓存会经历 **容量不命中(capacity miss)**。换句话说就是，缓存太小了，不能处理这个工作集。

> [!tip]+ 工作集
> 程序通常不是随机访问所有数据，而是阶段性地使用一部分数据，这部分被频繁访问的数据称为工作集。

### 4. 缓存管理
缓存不是自动凭空工作的，而是要有 “管理逻辑” 来判断：
- 哪些块在缓存中；
- 命中或不命中；
- 替换哪个块；
- 从哪一层取数据。

不同层次的缓存由不同组件管理：

| 层次            | 管理者           | 管理方式       |
| ------------- | ------------- | ---------- |
| CPU寄存器 ↔ L1缓存 | 编译器           | 决定寄存器分配与加载 |
| L1、L2、L3缓存    | 硬件逻辑          | 自动检测命中/替换  |
| 主存 ↔ 磁盘       | 操作系统 + 地址翻译硬件 | 管理虚拟内存页    |
| 本地磁盘 ↔ 网络文件系统 | 客户端软件（如 AFS）  | 以文件为单位缓存   |

## 6.3.2 存储器层次结构概念小结
存储器层次结构的设计核心在于 **平衡 “速度” 和 “成本”**。
- 快的存储设备（如寄存器、L1缓存）造价高、容量小；
- 慢的存储设备（如磁盘、远程服务器）造价低、容量大。
所以，系统通过 “层次结构” 来折中：**让上层的小而快的缓存，存放下层大而慢设备中最“常用”的部分数据**。而这能 “有效运行” 的关键原因，是 -- **局部性**：
- **利用时间局部性**：由于时间局部性，同一数据对象可能会被多次使用。一旦一个数据对象在第一次不命中时被复制到缓存中，如果后面对该目标有一系列的访问命中，因为缓存比低一层的存储设备更快，对后面的命中的服务会比最开始的不命中快很多，就能抵消第一次的成本；
- **利用空间局部性**：虽然每次不命中都要复制一整个块，但如果块内的多个对象被利用，整体上是划算的；

# 6.4 高速缓存存储器
早期计算机的存储层次结构只有三层：  **CPU寄存器 → 主存（DRAM） → 磁盘**。随着 **CPU 与主存速度差距扩大**，设计者在两者之间依次加入更快的缓存层，如图 6-24 所示：
- **L1缓存**：位于CPU和主存之间，访问延迟约4个时钟周期；
- **L2缓存**：比L1更大、更慢，约10个周期；
- **L3缓存**：再大一级，约50个周期。

![[NoteAboutStudy/attachments/Pasted image 20251109211728.png]]
> 每向下一层，容量增大、速度变慢、成本降低。

---
## 6.4.1 通用的高速缓存存储器组织结构
考虑一个计算机系统，其中每个存储器地址有 $m$ 位，形成 $M=2^m$ 个不同的地址。如图 6-25a 所示，一个缓存可以用四元组 **(S, E, B, m)** 描述：
- 缓存中有 $S=2^s$ 个 **高速缓存组(cache set)**；
- 每个组有 $E$ 个 **高速缓存行(cache line)**；
- 每个行是由一个 $B=2^b$ 字节的数据块(block) 组成的，还包含一个 **有效位(valid bit)** -- 表示该行算法存有有效数据，还有 $t=m-(b+s)$ 个 **标记位(tag bit)** -- 是当前块的内存地址的位的一个子集，用于唯一标识存储在这一行中的块；
![[NoteAboutStudy/attachments/Pasted image 20251109215546.png]]
> 缓存的总容量为：$C=S×E×B$（不含标记位与有效位，$C$ 指所有块的大小之和）


## 6.4.2 直接映射高速缓存
根据每个组的高速缓存行数E，高速缓存被分为不同的类。<u>每个组只有一行</u>（E=1）的高速缓存称为 **直接映射高速缓存（direct-mapped cache）**（见图6-27）。直接映射高速缓存是最容易实现和理解的，将以它为例来说明一些高速缓存工作方式的通用概念：
![[NoteAboutStudy/attachments/Pasted image 20251111205224.png]]

---
### 访问步骤
CPU读取内存字 $w$ 时，先向 L1 缓存请求：
- 若缓存中已有 $w$ 的副本 → **命中**，直接返回；
- 若没有 → **不命中**，从主存取块存入缓存后再返回；
高速缓存确定一个请求是否命中，然后抽取出被请求的字的过程分为三步：
1. **组选择**；
2. **行匹配**；
3. **字抽取**；

#### 1. 直接映射高速缓存中的组选择
高速缓存从 $w$ 的地址中间取出 $s$ 个 **组索引位**（6.4.1 中的），这些位代表着该地址映射到哪个组。如图 6-28 所示。所谓组索引位如同数组的索引一般。
![[NoteAboutStudy/attachments/Pasted image 20251111210432.png]]

#### 2. 直接映射高速缓存中的行匹配
当选择了某个组 $i$ 后，需要确定其中是否有一个高速缓存行存储有字 $w$ 的一个副本。这对于直接映射高速缓存来说很容易而且很快，因为它的每个组中只有一行高速缓存行，所以：
- 若标记相同且有效位为 1 → **命中**。
- 否则 → **不命中**。
如图 6-29 所示，展示了直接映射高速缓存中行匹配是如何工作的：
![[NoteAboutStudy/attachments/Pasted image 20251111211405.png]]

#### 3. 直接映射高速缓存中的字选择
一旦命中，就知道了 $w$ 在块中的位置，最后一步确定所需要的字在块中是从哪里开始的。 如图 6-29 所示，块偏移位提供了所需要的字的第一个字节的偏移。

#### 4. 直接映射高速缓存中不命中时的行替换
若不命中：从下一层存储器中取出对应的块，并替换当前组中那唯一的一行（因为E=1），把新块写入缓存并返回数据。

#### 5. 综合：运行中的直接映射高速缓存
该部分通过实例从初始高速缓存为空时开始一步步讲解，具体阅读书中该节(P429)。

### 直接映射高速缓存中的冲突不命中
当多个块映射到同一个组，且程序交替访问它们时，会导致**频繁替换与不命中**，称为冲突不命中。

示例：
```c
for (i = 0; i < N; i++)
    sum += x[i] * y[i];
```
若数组 `x` 和 `y` 的起始地址正好导致它们的块映射到同一组，则在循环中：
- 访问 `x[i]` → 加载块 `x[0..3]`；
- 接着访问 `y[i]` → 加载块 `y[0..3]` 覆盖前者；
- 再次访问 `x[i+1]` → 又不命中并重新加载前块。  
    这种反复加载与驱逐称为 **抖动 (thrashing)**。

---
为避免冲突不命中，可：
- 调整数据结构在内存中的布局；
- 在数组末尾添加 **填充（padding）**，打破“相同组映射”的对齐。  

例如：
```c
float x[12]; // 在 x 末尾增加填充
float y[8];
```
这样 `x` 与 `y` 的块会映射到不同的组，从而减少抖动。

> [!tip]- 为什么使用中间的位做数据索引
> 主存地址是一串二进制位。高速缓存要从这些位中选出一部分作为组索引（决定一个数据放在哪个组里）。
> 
> 1. 如果选高位做索引：地址相邻的数据（如数组的连续元素）在高位上几乎一样，只低位不同，这样很多相邻的块就被映射到同一个组。程序顺序访问数组时，每次访问都会把同一个组里的旧块替换掉新块，缓存利用率极低。
> 2. 如果选中间位做索引：地址相邻的数据在中间位上不同，所以连续的块会被映射到不同的组。顺序访问时，每个组都能保存一部分数组内容，缓存能保留更多有用数据。
> 
> 用中间位作为组索引能更好地支持顺序访问，提高命中率；用高位则会导致连续数据冲突，性能下降。取出对应的块，并替换当前组中那唯一的一行（因为E=1），把新块写入缓存并返回数据。

## 6.4.3 组相联高速缓存
直接映射高速缓存中冲突不命中造成的问题源于每个组只有一行（或者说 $E=1$）这个限制。而 **组相联高速缓存（set associative cache）** 放松了这条限制，所以每个组都保存有多于一个的高速缓存行。**一个 $1<E<C/B$ 的高速缓存通常称为 $E$ 路组相联高速缓存**。图 6-32 展示了一个 2 路组相联高速缓存的结构。

![[NoteAboutStudy/attachments/Pasted image 20251111221544.png]]

### 1. 组相联高速缓存中的组选择
与直接映射高速缓存的组选择一样，用组索引位来表示组。如图 6-33 所示。

![[NoteAboutStudy/attachments/Pasted image 20251111221734.png]]

### 2. 组相联高速缓存中的行匹配和字选择
在组相联高速缓存中需要检查多个行的标记位和有效位，以确定所请求的字是否在集合中。

传统的内存是一个值的数组，以地址作为输入，并返回存储在那个地址的值。另一方面，**相联存储器**是一个（key，value）对的数组，以 `key` 为输入，返回与输入的 `key` 相匹配的（key，value）对中的 `value` 值。因此，可以把组相联高速缓存中的每个组都看成一个小的相联存储器，`key` 是标记和有效位，而 `value` 就是块的内容。

![[NoteAboutStudy/attachments/Pasted image 20251111222150.png]]

如图 6-34 所示，组中的任何一行都可以包含任何映射到这个组的内存块。所以高速缓存必须搜索组中的每一行，寻找一个有效的行，其标记与地址中的标记相匹配。如果高速缓存找到了这样一行，那么则命中，然后块偏移从这个块中选择一个字，和直接映射高速缓存一样。

### 3. 组相联高速缓存中不命中时的行替换
1. 若组中无匹配行，则缓存不命中：
    - 若有空行 → 直接填入。
    - 若全满 → 按替换策略驱逐一行。
        
2. 替换策略：
    - **随机**：随机选择要替换的行，实现简单。
    - **LFU（最少使用）**：替换过去一段时间使用次数最少的行。
    - **LRU（最近最少使用）**：替换最近最久未被访问的行（常用）。
        
> 所有这些策略都需要额外的时间和硬件，但是越接近 CPU，策略越简单；越靠近主存，策略越复杂，因为一次不命中的开销越昂贵，所以更需要减少不命中。

## 6.4.4 全相联高速缓存
**全相联高速缓存（fully associative cache）** 是由一个包含所有高速缓存行的组（即 $E=C/B$ ）组成的。图 6-35 给出了基本结构。

![[NoteAboutStudy/attachments/Pasted image 20251111223131.png]]

### 1. 全相联高速缓存中的组选择
因为只有一个组，不需要选择组，并且地址中没有组索引位，地址只被划分成一个标记和一个块偏移，如图 6-36 所示。

![[NoteAboutStudy/attachments/Pasted image 20251111223440.png]]

### 2. 全相联高速缓存中的行匹配和字选择
与组相联高速缓存类似，但规模更大。如图 6-37 所示。

![[NoteAboutStudy/attachments/Pasted image 20251111223604.png]]

因为高速缓存电路必须并行地搜索许多相匹配的标记，构比较硬件复杂且成本高，构造大容量的全相联缓存很困难。因此，全相联高速缓存只适合做小型高速缓存，例如虚拟内存系统中的翻译备用缓冲器（TLB），用于缓存页表项（见 9.6.2 节）。

## 6.4.5 有关写的问题
假设要写入的字 $w$ 在缓存中存在副本（写命中，write hit），有两种策略：
1. **直写(write-through)**：
	- 每次写都立即将 $w$ 的高速缓存块同步更新到低一层存储；
	- 优点：实现简单，数据一致性好；
	- 缺点：频繁的写操作造成总线流量大，性能下降；
2. **写回(write-back)**：
	- 先只在缓存中修改数据，**暂不写入，尽可能地推迟更新**，当该块被替换时，再写到低一层中；
	- 优点：利用**时间局部性**，多次写同一块只写回一次，大幅减少总线传输；
	- 缺点：实现复杂，需要为每个高速缓存行设置一个 **修改位（dirty bit）** 标记是否修改过；

假设要写入的数据**不在缓存中**（写不命中，write miss），同样有两种策略：
1. **写分配(write-allocate)**：
	- 先加载相应的低一层中的块到高速缓存中，然后更新这个高速缓存块；
	- 优点：利用**空间局部性**，同一块可能很快又会被访问；
	- 缺点：每次不命中都要传整块数据上来，代价较高；
	- 常与“写回”组合使用 → **Write-Back + Write-Allocate**；
2. **非写分配(not-write-allocate)：**
	- 不加载到缓存，直接把数据写入低一层的存储中；
	- 优点：简单，节省缓存带宽；
	- 缺点：无法利用空间局部性；
	- 常与“直写”组合使用 → **Write-Through + Not-Write-Allocate**；

几乎所有层次的缓存（包括虚拟内存）都倾向于使用**写回 + 写分配**。原因：
 - 符合局部性原理（同一块常被反复访问）；  
 - 减少总线通信；
 - 写回复杂性随着芯片密度提升已可接受；

## 6.4.6 一个真实的高速缓存层次结构的解剖
到目前为止，一直假设高速缓存只保存程序数据。不过，实际上，现代 CPU 中缓存分为三种用途：
1. **指令缓存（i-cache）**
    - 只存储程序指令；
    - 只读，结构简单；
    - 针对**顺序访问模式**（程序代码）优化；
        
2. **数据缓存（d-cache）**
    - 只存储程序数据；
    - 支持频繁的读写操作；
    - 针对**随机访问模式**（变量、数组等）优化；
        
3. **统一的高速缓存（unified cache）**
    - 同时存储指令与数据；
    - 实现简单，但可能出现**指令与数据竞争同一块缓存**的问题；

现代处理器通常包括独立的 `i-cache` 和 `d-cache`。原因如下：
- 有两个独立的高速缓存，处理器能够同时读一个指令字和一个数据字，提高并行度；
- 通常会针对不同的访问模式来优化这两个高速缓存，它们可以有不同的块大小，相联度和容量；
- 使用不同的高速缓存也确保了数据访问不会与指令访问形成冲突不命中，反过来也是一样，**代价就是可能会引起容量不命中增加**（因为两块缓存独立计算容量）；

图 6-38 给出了 `Intel Core i7` 处理器的高速缓存层次结构。每个 CPU 芯片有四个核。每个核有自己私有的 `L1 i-cache`、`L1 d-cache` 和 `L2` 统一的高速缓存。所有的核共享片上 `L3` 统一的高速缓存。这个层次结构的一个有趣的特性是所有的 SRAM 高速缓存存储器都在 CPU 芯片上。

![[NoteAboutStudy/attachments/Pasted image 20251112212907.png]]

- **L1分离设计（i-cache + d-cache）**：提高并行度，降低结构冲突；
- **L2统一缓存**：折中容量与灵活性；
- **共享L3缓存**：支持多核间通信，减少重复数据副本；
- **SRAM全部在片上**：减少访问延迟，提升带宽；

## 6.4.7 高速缓存参数的性能影响
### 一、 基本性能指标
有许多指标来衡量高速缓存的性能：
- **不命中率(miss rate)**：在一个程序执行或程序的一部分执行期间，内存引用的不命中的比率，用 `不命中数量/引用数量`；
- **命中率(hit rate)**：命中的内存引用比例，可用 `1-不命中率` 来计算；
- **命中时间(hit time)**：命中时，从高速缓存传送一个字到 CPU 所需时间，包括组选择、含确认和字选择的时间（L1：几周期，L2：十几周期）；
- **不命中处罚(miss penalty)**：发生不命中时，等待下一级缓存或主存的额外时间（Ll不命中需要从L2得到服务的处罚，通常是数10个周期；从L3得到服务的处罚，50个周期；从主存得到的服务的处罚，200个周期）；

### 二、主要影响因素
#### 1. 高速缓存大小的影响
较大缓存可以容纳更多数据从而提高命中率，但是电路复杂度也会提高，从而导致访问延迟（命中时间）上升。
这也就是高层缓存更小、更快，底层缓存更大、更慢的原因。

#### 2. 块大小的影响
较大的块能利用程序中可能存在的空间局部性（一次加载更多相邻字），帮助提高命中率，但是：
- 对于固定大小的高速缓存来说，块越大，高速缓存行数越少，对于时间局部性比空间局部性更好的程序，会降低它们的命中率；
- 同时，块越大，传送时间越长，不命中处罚的时间会更长；
所以，现代系统(如 Core i7)会这种使高速缓存块包含 64 个字节。

#### 3. 相联度的影响
定义：每个组中包含的行数。
- 直接映射：E = 1
- 全相联：E = C/B
- 组相联：1 < E < C/B

较高的相联度能 **降低冲突不命中**，但是成本更高，而且很难使之速度变快：
- 每一行需要更多的标记位，每一行需要额外的 LRU 状态位和额外的控制逻辑；
- 复杂性更高，命中时间增加；
- 替换策略更复杂，不命中处罚增加；
因此，相联度的选择最终会变成命中时间和不命中处罚之间的折中。（传统上，努力争取时钟频率的高性能系统会为 L1 高速缓存选择较低的相联度，而在不命中处罚比较高的较低层上使用比较小的相联度。）

#### 4. 写策略的影响
> **越靠近 CPU → 注重速度与简单性 → 多用直写**；  
> **越靠近主存 → 注重带宽与效率 → 多用写回**；

原因：
- 直写高速缓存比较容易，且能使用独立于高速缓存的 **写缓存区(write buffer)** 用来更新内存；
- 读不命中开销没那么大，因为不会触发内存写，而且，写回高速缓存引起的传送比较少，节省带宽，可用让更多的到内存的带宽用于执行 DMA 的 I/O 设备，越往层次结构下层，传送时间增加，减少传送的数量就变得更加重要；

# 6.5 编写高速缓存友好的代码
良好的局部性 = 较低的不命中率 = 更高的执行速度。  
高速缓存友好的程序 = **尽可能重用缓存中的数据**，**按连续内存顺序访问数据**。

（同时，该节通过 6.2 节的代码实例去讲解如何编写高速缓存友好的代码。具体察看 P440）

## 一、编写高速缓存友好代码的两个基本策略
1. **让最常执行的部分运行得快**
    - 程序大多数时间集中在少数核心函数（尤其是循环）中；
    - 优化重点：循环体；
    - 其他部分影响较小，可忽略；
        
2. **减少循环中的缓存不命中次数**
    - 在加载次数相同的情况下，不命中率更低的循环执行更快；
    - 因此，访问模式和数据布局至关重要；

## 二、结论
- **时间局部性**：重复使用相同变量或数据。  
    → 局部变量应常驻寄存器或缓存。

- **空间局部性**：按连续内存顺序访问数据。  
    → 步长为1最佳，避免跳跃式访问。
    
- **二维数组**：遵循C语言的行优先存储方式。
    
- **优化重点**：聚焦在程序中最常执行的循环上。

# 6.6 综合：高速缓存对程序性能的影响
本节通过研究高速缓存对运行在实际机器上的程序的性能影响，综合了对存储器层次结构的讨论。

---
## 6.6.1 存储器山
**存储器山（memory mointain）** 是通过测量程序在不同工作集大小（size）和访问步长（stride）下的 **读吞吐量（read throughput）**，来可视化存储系统性能的一种方法。
- **读吞吐量** = 读出字节数 ÷ 时间（单位：MB/s） （也称为 **读带宽 read bandwidth**）。
- 它反映了时间局部性与空间局部性对性能的综合影响。

### 一、测量方法
使用紧密循环读取数组元素的程序，控制：
 - `size` → 决定工作集大小，影响**时间局部性**；    
- `stride` → 决定访问间隔，影响**空间局部性**。
多次测量后，可得二维函数：**读吞吐量 = f(size, stride)**，形成“存储器山”。

### 二、存储器山的结构
以 Intel Core i7 为例，如图 6-41：
1. **四条山脊**对应工作集完全在不同层级的存储器内的时间局部性区域：
    - L1、L2、L3、高速缓存 → 主存；
2. 吞吐量层级差异明显：L1 峰值约 14 GB/s，主存约 0.9 GB/s，相差一个数量级。
3. 对于步长为 1 的顺序访问，Core i7 的硬件预取器可提前将数据加载到缓存,因此步长为 1 的吞吐量在超出 L1/L2 后仍保持高水平（≈12 GB/s）
![[NoteAboutStudy/attachments/Pasted image 20251114142716.png]]

如图 6-42 所示：
- 当工作集大小刚好等于缓存容量时（如 256KB 对应 L2），出现**吞吐量下降**，原因可能是**缓存冲突不命中**。
- 当步长达到缓存块大小（如 8 个 long = 64B），每次访问均导致不命中 → 吞吐量恒定。

![[NoteAboutStudy/attachments/Pasted image 20251114151717.png]]

- **时间局部性**
    - 工作集较小时，数据可保存在高速缓存中；
    - 吞吐量高（如 L1 命中时接近峰值）。
        
- **空间局部性**
    - 步长越小，访问越连续；
    - 同一缓存块中可利用的数据越多；
    - 吞吐量随步长增大而平滑下降。

## 6.6.2 重新排列循环以提高空间局部性
矩阵乘法 $$C=AB$$
常用三重循环实现，共有 **6 种** 循环顺序（i-j-k 的 6 个排列）。虽然所有版本执行 **相同数量的算术操作（O(n³)）**，但性能差异巨大（可达 40 倍），根本原因在于：  
> **不同版本的最内层循环对 A/B/C 的访问方式不同 → 空间局部性差异 → 缓存不命中数不同 → CPU 时间不同**。

为了分析最内层循环，每次迭代的行为基于以下假设：
- double 类型数组（8 字节）；
- 缓存块大小 B = 32 字节（一个块 4 个 double）；
- n 很大到一行无法放入 L1；
- 局部变量在寄存器中（不产生额外访问）。

根据最内层访问的矩阵对，可以分为三个等价类，而且访问行（stride = 1）具有良好空间局部性；访问列（stride = n）几乎每次都不命中：

| 类别     | 最内层访问的矩阵    | 对应循环版本  |
| ------ | ----------- | ------- |
| **AB** | A（行） + B（列） | ijk、jik |
| **AC** | A（列） + C（列） | ikj、kij |
| **BC** | B（行） + C（行） | jki、kji |

**三个等价类的缓存行为**
1. 类 AB（访问 A 的行，访问 B 的列）
	- A：步长 1（按行），命中好。  
	    → 每次迭代约 **0.25 次不命中**（4 个 double/缓存块）
	- B：步长 n（按列），每次都不命中。  
	    → 每次迭代 **1 次不命中**
	- 不命中总数：  
	    **1.25 次/迭代**
> 访问 B 的列是瓶颈。

2. 类 AC（访问 A 的列 + C 的列）
	- A：步长 n → **每次都不命中**
	- C：步长 n → **每次都不命中**
	- 每次迭代加载 A、加载 C、写 C（写可能导致写分配 miss）
> **2 次不命中/迭代（最差）**，两个列访问叠加 → 空间局部性最差 → 整体性能最低。

3. 类 BC（访问 B 的行 + C 的行）
	- B：步长 1 → 0.25 次 miss/迭代
	- C：步长 1 → 0.25 次 miss/迭代
	- 每次迭代多一次存储操作（比 AB 多访问一次）
> **0.5 次/迭代（比 AB 少得多）**，虽然访问次数比 AB 多，但**缓存不命中更少 → 反而性能更好**。

### 结论
1. **循环顺序决定最内层循环的内存访问模式 → 决定缓存不命中次数 → 决定性能。**

2. **减少 stride（越接近 1 越好）是最关键的优化。**

3. 行访问优于列访问：
	- 行：一个 miss，可用 4 个 double；
	- 列：每次都 miss。

4. 最优顺序是让**B 和 C 的行在最内层顺序访问（stride = 1）**。

5. 硬件预取对顺序访问帮助巨大，对乱序/大步长无能为力。

> [!tip]- 使用分块来提高时间局部性
> **分块(blocking)**
> 
> 目标：通过把大型数据结构拆成能完全放入 L1 缓存的小片段（块），在块内部完成所有计算，提高时间局部性。
> 
> 方法：按块加载 → 在块内反复使用数据 → 用完丢弃块 → 加载下一块。
> （这里的“块”是应用级片段，不是缓存块。）
> 
> 作用：减少在大数据结构上的跨区域访问，让同一小片数据在缓存中被充分重复利用。
> 
> 代价：代码结构更复杂、可读性变差，所以更适用于：高性能库（如矩阵库），编译器自动优化
> 
> 平台差异：在像 Core i7 这种拥有强预取器的现代 CPU 上，blocking 对矩阵乘法的收益有限；在缺乏预取功能的系统上，blocking 能带来显著性能提升。

## 6.6.3 在程序中利用局部性
### 1. **专注内循环（inner loops）**
> 大部分计算和内存访问都在内循环里，优化其他地方收益微乎其微。

任何想提升性能的代码，都要先找出：
- 最常执行的循环
- 循环内存访问模式（stride, reuse）
- 循环中/哪个数组访问最频繁
优化必须从这些循环开始。

---
### 2. **尽量按内存中存储顺序（stride = 1）访问数据 → 最大化空间局部性**
现代所有存储层次（缓存、DRAM、TLB）都假设一个事实：
> “访问了一个地址，等会儿很可能访问它旁边的地址。”

因此：
- 按行遍历的二维数组比按列访问快几十倍
- 线性扫描数组远快于跳跃访问
- 结构体紧密排列比链式指针访问快很多

空间局部性的本质：**一次 miss 取来一整个缓存块，用得越多越值。**

---
### 3. **读入一个数据后，尽可能多用它 → 最大化时间局部性**
时间局部性意味着：
> “刚访问过的数据，很可能还要访问。”

所以优化策略是：
- 不要重复从内存加载同一个值
- 利用寄存器/缓存存住热点值
- 阻止无谓的中间值写回内存
- 避免不必要的数据结构拆分

# 6.7 小结
存储层次结构以 “快的小存储+慢的大存储” 实现性能与成本平衡，而程序员通过提升局部性可以最大化地利用高速缓存，从而显著加速程序。